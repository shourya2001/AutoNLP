{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sdv\n",
        "!pip uninstall numpy -y\n",
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n5BCpsb5h7p-",
        "outputId": "e0aa5e46-fb75-40a2-b95c-494c652d1fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sdv\n",
            "  Downloading sdv-0.16.0-py2.py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting Faker<10,>=3.0.0\n",
            "  Downloading Faker-9.9.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 28.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<2,>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from sdv) (1.3.5)\n",
            "Collecting deepecho<0.4,>=0.3.0.post1\n",
            "  Downloading deepecho-0.3.0.post1-py2.py3-none-any.whl (26 kB)\n",
            "Collecting sdmetrics<0.6,>=0.5.0\n",
            "  Downloading sdmetrics-0.5.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting copulas<0.8,>=0.7.0\n",
            "  Downloading copulas-0.7.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting rdt<0.7,>=0.6.2\n",
            "  Downloading rdt-0.6.4-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from sdv) (1.21.6)\n",
            "Collecting ctgan<0.6,>=0.5.1\n",
            "  Downloading ctgan-0.5.1-py2.py3-none-any.whl (24 kB)\n",
            "Collecting graphviz<1,>=0.13.2\n",
            "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5,>=4.15 in /usr/local/lib/python3.7/dist-packages (from sdv) (4.64.0)\n",
            "Requirement already satisfied: scipy<2,>=1.5.4 in /usr/local/lib/python3.7/dist-packages (from copulas<0.8,>=0.7.0->sdv) (1.7.3)\n",
            "Collecting matplotlib<4,>=3.4.0\n",
            "  Downloading matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2,>=0.24 in /usr/local/lib/python3.7/dist-packages (from ctgan<0.6,>=0.5.1->sdv) (1.0.2)\n",
            "Requirement already satisfied: torchvision<1,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from ctgan<0.6,>=0.5.1->sdv) (0.13.0+cu113)\n",
            "Requirement already satisfied: torch<2,>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from ctgan<0.6,>=0.5.1->sdv) (1.12.0+cu113)\n",
            "Requirement already satisfied: packaging<22,>=20 in /usr/local/lib/python3.7/dist-packages (from ctgan<0.6,>=0.5.1->sdv) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.2 in /usr/local/lib/python3.7/dist-packages (from Faker<10,>=3.0.0->sdv) (4.1.1)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.7/dist-packages (from Faker<10,>=3.0.0->sdv) (1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.7/dist-packages (from Faker<10,>=3.0.0->sdv) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=3.4.0->copulas<0.8,>=0.7.0->sdv) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=3.4.0->copulas<0.8,>=0.7.0->sdv) (1.4.4)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)\n",
            "\u001b[K     |████████████████████████████████| 944 kB 36.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=3.4.0->copulas<0.8,>=0.7.0->sdv) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=3.4.0->copulas<0.8,>=0.7.0->sdv) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.1.3->sdv) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.4->Faker<10,>=3.0.0->sdv) (1.15.0)\n",
            "Collecting pyyaml<6,>=5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 27.9 MB/s \n",
            "\u001b[?25hCollecting psutil<6,>=5.7\n",
            "  Downloading psutil-5.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[K     |████████████████████████████████| 281 kB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2,>=0.24->ctgan<0.6,>=0.5.1->sdv) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2,>=0.24->ctgan<0.6,>=0.5.1->sdv) (1.1.0)\n",
            "Collecting pyts<0.13.0,>=0.12.0\n",
            "  Downloading pyts-0.12.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.7/dist-packages (from pyts<0.13.0,>=0.12.0->sdmetrics<0.6,>=0.5.0->sdv) (0.56.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->pyts<0.13.0,>=0.12.0->sdmetrics<0.6,>=0.5.0->sdv) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->pyts<0.13.0,>=0.12.0->sdmetrics<0.6,>=0.5.0->sdv) (0.39.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->pyts<0.13.0,>=0.12.0->sdmetrics<0.6,>=0.5.0->sdv) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision<1,>=0.9.0->ctgan<0.6,>=0.5.1->sdv) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.48.0->pyts<0.13.0,>=0.12.0->sdmetrics<0.6,>=0.5.0->sdv) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision<1,>=0.9.0->ctgan<0.6,>=0.5.1->sdv) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision<1,>=0.9.0->ctgan<0.6,>=0.5.1->sdv) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision<1,>=0.9.0->ctgan<0.6,>=0.5.1->sdv) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision<1,>=0.9.0->ctgan<0.6,>=0.5.1->sdv) (1.24.3)\n",
            "Installing collected packages: fonttools, pyyaml, psutil, matplotlib, rdt, pyts, copulas, sdmetrics, graphviz, Faker, deepecho, ctgan, sdv\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed Faker-9.9.1 copulas-0.7.0 ctgan-0.5.1 deepecho-0.3.0.post1 fonttools-4.34.4 graphviz-0.20.1 matplotlib-3.5.2 psutil-5.9.1 pyts-0.12.0 pyyaml-5.4.1 rdt-0.6.4 sdmetrics-0.5.0 sdv-0.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.21.6\n",
            "Uninstalling numpy-1.21.6:\n",
            "  Successfully uninstalled numpy-1.21.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "Successfully installed numpy-1.21.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKtsqTeEocs6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv('/content/dataset_final.csv')\n",
        "df = pd.read_csv('/content/final_pred_model_dataset.csv')"
      ],
      "metadata": {
        "id": "EtZ9JOyjogtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "yZptVa3Oogwa",
        "outputId": "7252a706-c3a0-4996-dbd2-bc908e1605ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         Dataset    Size      NB  Bert_1  Bert_2  Albert_1  \\\n",
              "0                 airline_tweets   14640  0.6605  0.8350  0.8264    0.8346   \n",
              "1          amazon_product_review   22641  0.5990  0.9050  0.8804    0.8975   \n",
              "2             coronavirus_tweets   41159  0.3475  0.5939  0.4936    0.6493   \n",
              "3                   inshort_news    4817  0.8610  0.9442  0.9431    0.9454   \n",
              "4                disaster_tweets   11370  0.8520  0.9129  0.9136    0.9125   \n",
              "5                   nykaa_review  155544  0.7580  0.7743  0.7764    0.7564   \n",
              "6              product_sentiment    6364  0.6512  0.7038  0.7051    0.7083   \n",
              "7              clickbait_dataset    5000  0.9520  0.9811  0.9794    0.9834   \n",
              "8                      fake_news    4997  0.8510  0.9657  0.9531    0.9743   \n",
              "9         stock_market_sentiment    5791  0.7170  0.8046  0.7765    0.7859   \n",
              "10                  amazon_cells    1000  0.8550  0.9343  0.9086    0.9571   \n",
              "11                          yelp    1000  0.7800  0.9486  0.8571    0.9371   \n",
              "12                 imdb_labelled     748  0.8000  0.9313  0.8053    0.9504   \n",
              "13                      SMS_spam    5572  0.9668  0.9923  0.9933    0.9903   \n",
              "14                  drugs_review    4000  0.7475  0.8379  0.8321    0.8507   \n",
              "15            AWARE_Productivity    3774  0.6477  0.6768  0.6662    0.6571   \n",
              "16       AWARE_Social_Networking    3118  0.6731  0.6905  0.6914    0.7088   \n",
              "17           AWARE_comprehensive   10601  0.6605  0.6657  0.6868    0.6654   \n",
              "18       apple_twitter_sentiment    3886  0.7464  0.7980  0.7335    0.7748   \n",
              "19  Financial_Sentiment_Analysis    5842  0.6638  0.7927  0.7829    0.7946   \n",
              "\n",
              "    Albert_2  XLNet_1  XLNet_2  NB_time  Bert_1_time  Bert_2_time  \\\n",
              "0     0.8207   0.8189   0.8275   0.1351     501.1543     482.6537   \n",
              "1     0.8036   0.8854   0.9029   0.4730     500.5851     500.7417   \n",
              "2     0.5100   0.4500   0.5104   0.2484     511.7019     516.7167   \n",
              "3     0.8832   0.9217   0.9437   0.1785     321.2276     321.1591   \n",
              "4     0.9075   0.8989   0.9054   0.1672     507.3875     507.1221   \n",
              "5     0.7732   0.7696   0.7850   0.0902     496.3858     489.5603   \n",
              "6     0.6966   0.6881   0.7011   0.0924     395.4590     376.2375   \n",
              "7     0.9749   0.9863   0.9891   0.0552     318.4037     303.2988   \n",
              "8     0.9788   0.9748   0.9834   1.4551     344.8036     330.1537   \n",
              "9     0.6956   0.7242   0.7568   0.0648     355.2241     355.0215   \n",
              "10    0.8200   0.6229   0.7257   0.0223      82.8036      65.3667   \n",
              "11    0.8286   0.6514   0.7743   0.0130      65.7418      65.8002   \n",
              "12    0.7939   0.6603   0.8397   0.0168      49.8459      49.6989   \n",
              "13    0.9887   0.9903   0.9949   0.1509     360.1390     344.0291   \n",
              "14    0.8271   0.8371   0.8586   0.1728     259.3465     259.3105   \n",
              "15    0.6684   0.6548   0.6896   0.2691     270.9299     268.8122   \n",
              "16    0.7060   0.7051   0.6969   0.2002     225.1325     224.9217   \n",
              "17    0.6929   0.6879   0.6882   0.8331     570.7930     554.1572   \n",
              "18    0.7080   0.7305   0.7568   0.0511     247.9680     247.3156   \n",
              "19    0.7531   0.7980   0.8127   0.0865     377.6815     376.3482   \n",
              "\n",
              "    Albert_1_time  Albert_2_time  XLNet_1_time  XLNet_2_time  Unnamed: 16  \\\n",
              "0        500.8666       300.4958      439.5199      430.3279          NaN   \n",
              "1        518.0940       311.4551      448.8743      448.9021          NaN   \n",
              "2        529.1710       316.9867      463.6896      454.1920          NaN   \n",
              "3        326.7332       196.3462      279.2409      279.2238          NaN   \n",
              "4        519.9831       312.1041      445.3390      445.5262          NaN   \n",
              "5        502.3225       301.3728      445.9857      436.5006          NaN   \n",
              "6        391.1370       235.0882      347.7264      339.2447          NaN   \n",
              "7        313.8886       187.4957      277.2519      268.4644          NaN   \n",
              "8        336.6970       202.6844      296.5957      287.9104          NaN   \n",
              "9        365.5839       220.1775      314.0790      314.1275          NaN   \n",
              "10        66.6647        39.4460       68.5243       57.1128          NaN   \n",
              "11        65.5250        39.4208       57.3823       57.3995          NaN   \n",
              "12        49.1416        29.8269       43.6560       43.6186          NaN   \n",
              "13       357.0584       213.9472      317.3053      308.6952          NaN   \n",
              "14       265.8833       159.8713      230.8605      231.0706          NaN   \n",
              "15       273.9679       164.4181      237.6869      233.9377          NaN   \n",
              "16       228.0807       137.1849      193.4793      193.3680          NaN   \n",
              "17       563.5796       338.1649      488.5069      478.4724          NaN   \n",
              "18       251.1960       150.7533      215.9294      216.0073          NaN   \n",
              "19       383.8538       230.3472      329.5942      329.1827          NaN   \n",
              "\n",
              "    best accuracy  Unnamed: 18  best time  \n",
              "0               1          NaN   300.4958  \n",
              "1               1          NaN   311.4551  \n",
              "2               3          NaN   316.9867  \n",
              "3               3          NaN   196.3462  \n",
              "4               2          NaN   312.1041  \n",
              "5               6          NaN   301.3728  \n",
              "6               3          NaN   235.0882  \n",
              "7               6          NaN   187.4957  \n",
              "8               6          NaN   202.6844  \n",
              "9               1          NaN   220.1775  \n",
              "10              3          NaN    39.4460  \n",
              "11              1          NaN    39.4208  \n",
              "12              3          NaN    29.8269  \n",
              "13              6          NaN   213.9472  \n",
              "14              6          NaN   159.8713  \n",
              "15              6          NaN   164.4181  \n",
              "16              3          NaN   137.1849  \n",
              "17              4          NaN   338.1649  \n",
              "18              1          NaN   150.7533  \n",
              "19              6          NaN   230.3472  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5d3ffe08-8ddf-46d0-9c7d-23b205dbc559\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th>Size</th>\n",
              "      <th>NB</th>\n",
              "      <th>Bert_1</th>\n",
              "      <th>Bert_2</th>\n",
              "      <th>Albert_1</th>\n",
              "      <th>Albert_2</th>\n",
              "      <th>XLNet_1</th>\n",
              "      <th>XLNet_2</th>\n",
              "      <th>NB_time</th>\n",
              "      <th>Bert_1_time</th>\n",
              "      <th>Bert_2_time</th>\n",
              "      <th>Albert_1_time</th>\n",
              "      <th>Albert_2_time</th>\n",
              "      <th>XLNet_1_time</th>\n",
              "      <th>XLNet_2_time</th>\n",
              "      <th>Unnamed: 16</th>\n",
              "      <th>best accuracy</th>\n",
              "      <th>Unnamed: 18</th>\n",
              "      <th>best time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>airline_tweets</td>\n",
              "      <td>14640</td>\n",
              "      <td>0.6605</td>\n",
              "      <td>0.8350</td>\n",
              "      <td>0.8264</td>\n",
              "      <td>0.8346</td>\n",
              "      <td>0.8207</td>\n",
              "      <td>0.8189</td>\n",
              "      <td>0.8275</td>\n",
              "      <td>0.1351</td>\n",
              "      <td>501.1543</td>\n",
              "      <td>482.6537</td>\n",
              "      <td>500.8666</td>\n",
              "      <td>300.4958</td>\n",
              "      <td>439.5199</td>\n",
              "      <td>430.3279</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>300.4958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>amazon_product_review</td>\n",
              "      <td>22641</td>\n",
              "      <td>0.5990</td>\n",
              "      <td>0.9050</td>\n",
              "      <td>0.8804</td>\n",
              "      <td>0.8975</td>\n",
              "      <td>0.8036</td>\n",
              "      <td>0.8854</td>\n",
              "      <td>0.9029</td>\n",
              "      <td>0.4730</td>\n",
              "      <td>500.5851</td>\n",
              "      <td>500.7417</td>\n",
              "      <td>518.0940</td>\n",
              "      <td>311.4551</td>\n",
              "      <td>448.8743</td>\n",
              "      <td>448.9021</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>311.4551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>coronavirus_tweets</td>\n",
              "      <td>41159</td>\n",
              "      <td>0.3475</td>\n",
              "      <td>0.5939</td>\n",
              "      <td>0.4936</td>\n",
              "      <td>0.6493</td>\n",
              "      <td>0.5100</td>\n",
              "      <td>0.4500</td>\n",
              "      <td>0.5104</td>\n",
              "      <td>0.2484</td>\n",
              "      <td>511.7019</td>\n",
              "      <td>516.7167</td>\n",
              "      <td>529.1710</td>\n",
              "      <td>316.9867</td>\n",
              "      <td>463.6896</td>\n",
              "      <td>454.1920</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>316.9867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>inshort_news</td>\n",
              "      <td>4817</td>\n",
              "      <td>0.8610</td>\n",
              "      <td>0.9442</td>\n",
              "      <td>0.9431</td>\n",
              "      <td>0.9454</td>\n",
              "      <td>0.8832</td>\n",
              "      <td>0.9217</td>\n",
              "      <td>0.9437</td>\n",
              "      <td>0.1785</td>\n",
              "      <td>321.2276</td>\n",
              "      <td>321.1591</td>\n",
              "      <td>326.7332</td>\n",
              "      <td>196.3462</td>\n",
              "      <td>279.2409</td>\n",
              "      <td>279.2238</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>196.3462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>disaster_tweets</td>\n",
              "      <td>11370</td>\n",
              "      <td>0.8520</td>\n",
              "      <td>0.9129</td>\n",
              "      <td>0.9136</td>\n",
              "      <td>0.9125</td>\n",
              "      <td>0.9075</td>\n",
              "      <td>0.8989</td>\n",
              "      <td>0.9054</td>\n",
              "      <td>0.1672</td>\n",
              "      <td>507.3875</td>\n",
              "      <td>507.1221</td>\n",
              "      <td>519.9831</td>\n",
              "      <td>312.1041</td>\n",
              "      <td>445.3390</td>\n",
              "      <td>445.5262</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>312.1041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>nykaa_review</td>\n",
              "      <td>155544</td>\n",
              "      <td>0.7580</td>\n",
              "      <td>0.7743</td>\n",
              "      <td>0.7764</td>\n",
              "      <td>0.7564</td>\n",
              "      <td>0.7732</td>\n",
              "      <td>0.7696</td>\n",
              "      <td>0.7850</td>\n",
              "      <td>0.0902</td>\n",
              "      <td>496.3858</td>\n",
              "      <td>489.5603</td>\n",
              "      <td>502.3225</td>\n",
              "      <td>301.3728</td>\n",
              "      <td>445.9857</td>\n",
              "      <td>436.5006</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>301.3728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>product_sentiment</td>\n",
              "      <td>6364</td>\n",
              "      <td>0.6512</td>\n",
              "      <td>0.7038</td>\n",
              "      <td>0.7051</td>\n",
              "      <td>0.7083</td>\n",
              "      <td>0.6966</td>\n",
              "      <td>0.6881</td>\n",
              "      <td>0.7011</td>\n",
              "      <td>0.0924</td>\n",
              "      <td>395.4590</td>\n",
              "      <td>376.2375</td>\n",
              "      <td>391.1370</td>\n",
              "      <td>235.0882</td>\n",
              "      <td>347.7264</td>\n",
              "      <td>339.2447</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>235.0882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>clickbait_dataset</td>\n",
              "      <td>5000</td>\n",
              "      <td>0.9520</td>\n",
              "      <td>0.9811</td>\n",
              "      <td>0.9794</td>\n",
              "      <td>0.9834</td>\n",
              "      <td>0.9749</td>\n",
              "      <td>0.9863</td>\n",
              "      <td>0.9891</td>\n",
              "      <td>0.0552</td>\n",
              "      <td>318.4037</td>\n",
              "      <td>303.2988</td>\n",
              "      <td>313.8886</td>\n",
              "      <td>187.4957</td>\n",
              "      <td>277.2519</td>\n",
              "      <td>268.4644</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>187.4957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>fake_news</td>\n",
              "      <td>4997</td>\n",
              "      <td>0.8510</td>\n",
              "      <td>0.9657</td>\n",
              "      <td>0.9531</td>\n",
              "      <td>0.9743</td>\n",
              "      <td>0.9788</td>\n",
              "      <td>0.9748</td>\n",
              "      <td>0.9834</td>\n",
              "      <td>1.4551</td>\n",
              "      <td>344.8036</td>\n",
              "      <td>330.1537</td>\n",
              "      <td>336.6970</td>\n",
              "      <td>202.6844</td>\n",
              "      <td>296.5957</td>\n",
              "      <td>287.9104</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>202.6844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>stock_market_sentiment</td>\n",
              "      <td>5791</td>\n",
              "      <td>0.7170</td>\n",
              "      <td>0.8046</td>\n",
              "      <td>0.7765</td>\n",
              "      <td>0.7859</td>\n",
              "      <td>0.6956</td>\n",
              "      <td>0.7242</td>\n",
              "      <td>0.7568</td>\n",
              "      <td>0.0648</td>\n",
              "      <td>355.2241</td>\n",
              "      <td>355.0215</td>\n",
              "      <td>365.5839</td>\n",
              "      <td>220.1775</td>\n",
              "      <td>314.0790</td>\n",
              "      <td>314.1275</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>220.1775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>amazon_cells</td>\n",
              "      <td>1000</td>\n",
              "      <td>0.8550</td>\n",
              "      <td>0.9343</td>\n",
              "      <td>0.9086</td>\n",
              "      <td>0.9571</td>\n",
              "      <td>0.8200</td>\n",
              "      <td>0.6229</td>\n",
              "      <td>0.7257</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>82.8036</td>\n",
              "      <td>65.3667</td>\n",
              "      <td>66.6647</td>\n",
              "      <td>39.4460</td>\n",
              "      <td>68.5243</td>\n",
              "      <td>57.1128</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>39.4460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>yelp</td>\n",
              "      <td>1000</td>\n",
              "      <td>0.7800</td>\n",
              "      <td>0.9486</td>\n",
              "      <td>0.8571</td>\n",
              "      <td>0.9371</td>\n",
              "      <td>0.8286</td>\n",
              "      <td>0.6514</td>\n",
              "      <td>0.7743</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>65.7418</td>\n",
              "      <td>65.8002</td>\n",
              "      <td>65.5250</td>\n",
              "      <td>39.4208</td>\n",
              "      <td>57.3823</td>\n",
              "      <td>57.3995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>39.4208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>imdb_labelled</td>\n",
              "      <td>748</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.9313</td>\n",
              "      <td>0.8053</td>\n",
              "      <td>0.9504</td>\n",
              "      <td>0.7939</td>\n",
              "      <td>0.6603</td>\n",
              "      <td>0.8397</td>\n",
              "      <td>0.0168</td>\n",
              "      <td>49.8459</td>\n",
              "      <td>49.6989</td>\n",
              "      <td>49.1416</td>\n",
              "      <td>29.8269</td>\n",
              "      <td>43.6560</td>\n",
              "      <td>43.6186</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29.8269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>SMS_spam</td>\n",
              "      <td>5572</td>\n",
              "      <td>0.9668</td>\n",
              "      <td>0.9923</td>\n",
              "      <td>0.9933</td>\n",
              "      <td>0.9903</td>\n",
              "      <td>0.9887</td>\n",
              "      <td>0.9903</td>\n",
              "      <td>0.9949</td>\n",
              "      <td>0.1509</td>\n",
              "      <td>360.1390</td>\n",
              "      <td>344.0291</td>\n",
              "      <td>357.0584</td>\n",
              "      <td>213.9472</td>\n",
              "      <td>317.3053</td>\n",
              "      <td>308.6952</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>213.9472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>drugs_review</td>\n",
              "      <td>4000</td>\n",
              "      <td>0.7475</td>\n",
              "      <td>0.8379</td>\n",
              "      <td>0.8321</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>0.8271</td>\n",
              "      <td>0.8371</td>\n",
              "      <td>0.8586</td>\n",
              "      <td>0.1728</td>\n",
              "      <td>259.3465</td>\n",
              "      <td>259.3105</td>\n",
              "      <td>265.8833</td>\n",
              "      <td>159.8713</td>\n",
              "      <td>230.8605</td>\n",
              "      <td>231.0706</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>159.8713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>AWARE_Productivity</td>\n",
              "      <td>3774</td>\n",
              "      <td>0.6477</td>\n",
              "      <td>0.6768</td>\n",
              "      <td>0.6662</td>\n",
              "      <td>0.6571</td>\n",
              "      <td>0.6684</td>\n",
              "      <td>0.6548</td>\n",
              "      <td>0.6896</td>\n",
              "      <td>0.2691</td>\n",
              "      <td>270.9299</td>\n",
              "      <td>268.8122</td>\n",
              "      <td>273.9679</td>\n",
              "      <td>164.4181</td>\n",
              "      <td>237.6869</td>\n",
              "      <td>233.9377</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>164.4181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>AWARE_Social_Networking</td>\n",
              "      <td>3118</td>\n",
              "      <td>0.6731</td>\n",
              "      <td>0.6905</td>\n",
              "      <td>0.6914</td>\n",
              "      <td>0.7088</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.7051</td>\n",
              "      <td>0.6969</td>\n",
              "      <td>0.2002</td>\n",
              "      <td>225.1325</td>\n",
              "      <td>224.9217</td>\n",
              "      <td>228.0807</td>\n",
              "      <td>137.1849</td>\n",
              "      <td>193.4793</td>\n",
              "      <td>193.3680</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>137.1849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>AWARE_comprehensive</td>\n",
              "      <td>10601</td>\n",
              "      <td>0.6605</td>\n",
              "      <td>0.6657</td>\n",
              "      <td>0.6868</td>\n",
              "      <td>0.6654</td>\n",
              "      <td>0.6929</td>\n",
              "      <td>0.6879</td>\n",
              "      <td>0.6882</td>\n",
              "      <td>0.8331</td>\n",
              "      <td>570.7930</td>\n",
              "      <td>554.1572</td>\n",
              "      <td>563.5796</td>\n",
              "      <td>338.1649</td>\n",
              "      <td>488.5069</td>\n",
              "      <td>478.4724</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>338.1649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>apple_twitter_sentiment</td>\n",
              "      <td>3886</td>\n",
              "      <td>0.7464</td>\n",
              "      <td>0.7980</td>\n",
              "      <td>0.7335</td>\n",
              "      <td>0.7748</td>\n",
              "      <td>0.7080</td>\n",
              "      <td>0.7305</td>\n",
              "      <td>0.7568</td>\n",
              "      <td>0.0511</td>\n",
              "      <td>247.9680</td>\n",
              "      <td>247.3156</td>\n",
              "      <td>251.1960</td>\n",
              "      <td>150.7533</td>\n",
              "      <td>215.9294</td>\n",
              "      <td>216.0073</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.7533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Financial_Sentiment_Analysis</td>\n",
              "      <td>5842</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.7927</td>\n",
              "      <td>0.7829</td>\n",
              "      <td>0.7946</td>\n",
              "      <td>0.7531</td>\n",
              "      <td>0.7980</td>\n",
              "      <td>0.8127</td>\n",
              "      <td>0.0865</td>\n",
              "      <td>377.6815</td>\n",
              "      <td>376.3482</td>\n",
              "      <td>383.8538</td>\n",
              "      <td>230.3472</td>\n",
              "      <td>329.5942</td>\n",
              "      <td>329.1827</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>230.3472</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d3ffe08-8ddf-46d0-9c7d-23b205dbc559')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5d3ffe08-8ddf-46d0-9c7d-23b205dbc559 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5d3ffe08-8ddf-46d0-9c7d-23b205dbc559');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "s = StandardScaler()\n",
        "\n",
        "df = df[['Size','NB','Albert_2','best accuracy']]\n",
        "df.rename(columns = {'best accuracy':'target'}, inplace = True)\n",
        "\n",
        "df[['Size', 'NB', 'Albert_2']] = s.fit_transform(df[['Size', 'NB', 'Albert_2']])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5qHrNs4TXK3j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "edaf2754-a302-48cd-be6e-112d2941bddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:5047: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3678: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[col] = igetitem(value, i)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Size        NB  Albert_2  target\n",
              "0 -0.170559 -0.214897  0.369069       1\n",
              "1 -0.119226 -0.531848  0.271532       1\n",
              "2 -0.000417 -1.827998 -1.403138       3\n",
              "3 -0.233583  0.818416  0.725564       3\n",
              "4 -0.191539  0.772033  0.864169       2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-31a1f500-dbf5-429a-9a63-9912b9f54680\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Size</th>\n",
              "      <th>NB</th>\n",
              "      <th>Albert_2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.170559</td>\n",
              "      <td>-0.214897</td>\n",
              "      <td>0.369069</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.119226</td>\n",
              "      <td>-0.531848</td>\n",
              "      <td>0.271532</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.000417</td>\n",
              "      <td>-1.827998</td>\n",
              "      <td>-1.403138</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.233583</td>\n",
              "      <td>0.818416</td>\n",
              "      <td>0.725564</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.191539</td>\n",
              "      <td>0.772033</td>\n",
              "      <td>0.864169</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31a1f500-dbf5-429a-9a63-9912b9f54680')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-31a1f500-dbf5-429a-9a63-9912b9f54680 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-31a1f500-dbf5-429a-9a63-9912b9f54680');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['target'].value_counts().plot(kind='bar', title='Count (target)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "WVEEbkcb4TB7",
        "outputId": "b9edb2e6-18a7-4286-a5f1-b53141a4b8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:title={'center':'Count (target)'}>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df.target != 2]\n",
        "df = df[df.target != 4]\n",
        "df = df[df.target != 5]\n",
        "\n",
        "df['target'].value_counts().plot(kind='bar', title='Count (target)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "kpHWW9lPqyTC",
        "outputId": "704506e0-42fd-4e64-b899-45ad9f49e585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:title={'center':'Count (target)'}>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['Size', 'NB', 'Albert_2']], df['target'], test_size=8, random_state=4)"
      ],
      "metadata": {
        "id": "sXgV_i_zC_uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts().plot(kind='bar', title='Count (target)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hDGwiDCr5O7X",
        "outputId": "3c16443a-58f7-4da1-d84f-f35bc227e5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:title={'center':'Count (target)'}>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train = le.transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "df['target'] = le.transform(df['target'])\n",
        "pd.Series(y_train)"
      ],
      "metadata": {
        "id": "G7dQ6rzG-hBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "017645dd-1a12-4329-ad8b-9e759ed02818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     0\n",
              "1     1\n",
              "2     1\n",
              "3     0\n",
              "4     2\n",
              "5     0\n",
              "6     0\n",
              "7     2\n",
              "8     0\n",
              "9     0\n",
              "10    2\n",
              "11    2\n",
              "12    2\n",
              "13    0\n",
              "14    0\n",
              "15    2\n",
              "16    1\n",
              "17    0\n",
              "18    1\n",
              "19    2\n",
              "20    2\n",
              "21    2\n",
              "22    1\n",
              "23    1\n",
              "24    0\n",
              "25    1\n",
              "26    2\n",
              "27    1\n",
              "28    0\n",
              "29    1\n",
              "30    0\n",
              "31    1\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y_train).value_counts().plot(kind='bar', title='Count (target)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "UoWonbaCtGgO",
        "outputId": "60684807-ee98-4047-de98-b4572f0a5042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:title={'center':'Count (target)'}>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def get_model(n_inputs, n_outputs):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(32, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
        "  model.add(Dense(16, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
        "  model.add(Dense(8, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
        "  model.add(Dense(n_outputs, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "  return model"
      ],
      "metadata": {
        "id": "xFNJ9y7-swfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import statistics\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "models = [\n",
        "    LogisticRegression(multi_class='multinomial'),\n",
        "    XGBClassifier(\n",
        "        max_depth=6,\n",
        "        gamma=2,\n",
        "        eta=0.8,\n",
        "        reg_alpha=0.5,\n",
        "        reg_lambda=0.5\n",
        "      ),\n",
        "    # LogisticRegression(),\n",
        "    svm.SVC(decision_function_shape='ovr', probability=True),\n",
        "    RandomForestClassifier(n_estimators=10),\n",
        "    AdaBoostClassifier(n_estimators=25, learning_rate=2),\n",
        "    get_model(X_train.shape[1], 1)\n",
        "]\n",
        "\n",
        "# preds = pd.DataFrame()\n",
        "acc_arr = []\n",
        "for i in range(10):\n",
        "  preds = []\n",
        "  weights = [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "  for i, m in enumerate(models):\n",
        "    if i == len(models)-1:\n",
        "      m.fit(X_train, y_train, epochs = 50)\n",
        "      preds.append(m.predict(X_test[['Size','NB', 'Albert_2']]) * weights[i])\n",
        "    else:\n",
        "      m.fit(X_train, y_train)\n",
        "      preds.append(m.predict_proba(X_test[['Size','NB', 'Albert_2']]) * weights[i])\n",
        "\n",
        "\n",
        "  preds_ensemble = [sum(items)/sum(weights) for items in zip(*preds)]\n",
        "  preds_ensemble = np.argmax(preds_ensemble, axis=1)\n",
        "  # preds_ensemble = le.inverse_transform(preds_ensemble)\n",
        "  print('Pred:', list(preds_ensemble))\n",
        "  acc = accuracy_score(y_test, preds_ensemble)\n",
        "  print('Real:', list(y_test))\n",
        "  print(acc)\n",
        "  acc_arr.append(acc)\n",
        "\n",
        "print(\"Average accuracy:\", statistics.mean(acc_arr))\n",
        "                 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvnFzTlE_dAq",
        "outputId": "25a2a0f1-b2c4-4cfe-b7ef-9cd8ca7095c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 2.9696\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8977\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8271\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.7576\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6892\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6224\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5571\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.4933\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4302\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.3678\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.3064\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2465\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1889\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1327\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0774\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0235\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.9707\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.9188\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.8681\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.8181\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.7704\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7256\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.6813\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.6387\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.5968\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.5553\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5145\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4744\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4362\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.3989\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.3627\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3284\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2975\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2683\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.2383\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.2075\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1770\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1464\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1155\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0852\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0553\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0253\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.9956\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.9669\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.9391\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9118\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.8849\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.8584\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8326\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.8072\n",
            "Pred: [2, 0, 2, 0, 1, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.625\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7824\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7580\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.7343\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7116\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6895\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6680\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6469\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6261\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6055\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5854\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5656\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5473\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5317\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5166\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5016\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4871\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4729\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4589\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4455\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4326\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4199\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4075\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3953\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3833\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3716\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3601\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3491\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3383\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3276\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3174\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3073\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2973\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2875\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2780\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2688\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2598\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2508\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2419\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2332\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2248\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2165\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2084\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2003\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1922\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1841\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1762\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1680\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1598\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1515\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1431\n",
            "Pred: [2, 0, 2, 0, 2, 0, 0, 1]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.5\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1342\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1251\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1158\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1065\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0972\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0879\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0786\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0695\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0604\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0513\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0423\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0332\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0241\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0158\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0075\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -8.7094e-04\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -0.0093\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.0178\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.0263\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.0348\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.0434\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -0.0521\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.0608\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -0.0695\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -0.0782\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.0870\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.0958\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.1048\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.1139\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.1230\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.1322\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -0.1415\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.1509\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.1604\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.1699\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.1796\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.1894\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.1993\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.2093\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.2191\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.2291\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -0.2391\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: -0.2496\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -0.2611\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: -0.2730\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: -0.2855\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -0.2983\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -0.3115\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -0.3250\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -0.3388\n",
            "Pred: [1, 0, 0, 0, 1, 2, 0, 1]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.25\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -0.3528\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -0.3670\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.3815\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -0.3961\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -0.4110\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -0.4260\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.4412\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.4565\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.4720\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.4877\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: -0.5036\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -0.5197\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: -0.5360\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: -0.5524\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -0.5691\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: -0.5859\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.6029\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.6200\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.6374\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.6542\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -0.6714\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -0.6892\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -0.7073\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -0.7256\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.7437\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.7622\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.7808\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -0.7997\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -0.8188\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -0.8372\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.8560\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.8751\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.8945\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.9143\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -0.9343\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.9546\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -0.9752\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -0.9961\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.0171\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.0384\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -1.0600\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -1.0819\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -1.1041\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -1.1266\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: -1.1493\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: -1.1722\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -1.1953\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: -1.2191\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: -1.2432\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -1.2676\n",
            "Pred: [1, 0, 2, 2, 1, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.625\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: -1.2922\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -1.3171\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -1.3424\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.3682\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -1.3944\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -1.4209\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -1.4478\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -1.4750\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -1.5026\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -1.5306\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -1.5590\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.5879\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -1.6171\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -1.6469\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.6770\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.7076\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.7386\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.7701\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.8020\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.8343\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -1.8671\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.9002\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.9339\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -1.9680\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -2.0026\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -2.0377\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.0733\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.1094\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -2.1461\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.1831\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.2209\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.2589\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.2975\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.3369\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -2.3770\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.4177\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.4589\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -2.5008\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -2.5434\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.5866\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.6302\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -2.6745\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -2.7196\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -2.7650\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -2.8110\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.8579\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.9054\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -2.9536\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -3.0024\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -3.0518\n",
            "Pred: [2, 0, 2, 0, 2, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.625\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -3.1020\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -3.1528\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -3.2045\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: -3.2569\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: -3.3100\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: -3.3638\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -3.4184\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -3.4738\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -3.5298\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -3.5865\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -3.6442\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -3.7027\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -3.7619\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -3.8220\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -3.8829\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -3.9447\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: -4.0073\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -4.0708\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -4.1352\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -4.2000\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -4.2659\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -4.3327\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -4.4007\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -4.4728\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -4.5469\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -4.6228\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -4.7002\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -4.7790\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -4.8594\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -4.9416\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -5.0263\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -5.1125\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -5.2005\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -5.2912\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -5.3861\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -5.4831\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -5.5820\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -5.6827\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -5.7814\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -5.8753\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -5.9708\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: -6.0680\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -6.1667\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -6.2670\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: -6.3690\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -6.4728\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -6.5786\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: -6.6861\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -6.7951\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -6.9055\n",
            "Pred: [1, 0, 0, 0, 1, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.5\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -7.0174\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -7.1307\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -7.2455\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -7.3618\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -7.4796\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -7.5990\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -7.7200\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: -7.8420\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -7.9662\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -8.0923\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -8.2195\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -8.3484\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -8.4792\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -8.6117\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -8.7457\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -8.8813\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -9.0187\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -9.1579\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -9.2990\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -9.4419\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -9.5865\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -9.7331\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -9.8815\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -10.0321\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -10.1849\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -10.3391\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -10.4955\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -10.6540\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -10.8143\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -10.9766\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -11.1411\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -11.3072\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -11.4755\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: -11.6460\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -11.8184\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -11.9929\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: -12.1697\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: -12.3480\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -12.5284\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -12.7107\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -12.8953\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -13.0818\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -13.2709\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -13.4622\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -13.6554\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -13.8515\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -14.0498\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -14.2503\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -14.4525\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -14.6573\n",
            "Pred: [2, 0, 0, 0, 1, 2, 0, 1]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.25\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -14.8641\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -15.0735\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -15.2857\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -15.5008\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -15.7191\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -15.9398\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -16.1627\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -16.3897\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -16.6184\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -16.8482\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -17.0815\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -17.3175\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -17.5562\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -17.7981\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -18.0428\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -18.2903\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -18.5413\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -18.7954\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -19.0535\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -19.3139\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -19.5769\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -19.8432\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -20.1120\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -20.3843\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -20.6599\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -20.9387\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -21.2210\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -21.5055\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -21.7940\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -22.0862\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -22.3804\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -22.6773\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -22.9775\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -23.2810\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -23.5874\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -23.8975\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -24.2106\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -24.5274\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -24.8472\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -25.1701\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -25.4964\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -25.8264\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -26.1585\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -26.4935\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -26.8314\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -27.1736\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -27.5191\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -27.8672\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -28.2192\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -28.5750\n",
            "Pred: [2, 0, 0, 0, 2, 2, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.375\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -28.9352\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -29.2977\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -29.6638\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -30.0335\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -30.4066\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -30.7829\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -31.1631\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -31.5476\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -31.9356\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -32.3272\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -32.7229\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -33.1231\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -33.5266\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: -33.9340\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: -34.3451\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -34.7594\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 44ms/step - loss: -35.1786\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -35.6026\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: -36.0292\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -36.4586\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -36.8934\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -37.3322\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -37.7752\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -38.2220\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -38.6744\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: -39.1303\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: -39.5902\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -40.0536\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -40.5212\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: -40.9936\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -41.4703\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: -41.9526\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: -42.4384\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -42.9287\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: -43.4243\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -43.9245\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -44.4311\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -44.9430\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -45.4605\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -45.9825\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -46.5072\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -47.0380\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -47.5731\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -48.1107\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -48.6545\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -49.2037\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: -49.7564\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -50.3136\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -50.8757\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -51.4424\n",
            "Pred: [1, 0, 0, 0, 1, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.5\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: -52.0147\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -52.5921\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -53.1744\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -53.7617\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -54.3562\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -54.9574\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -55.5629\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -56.1744\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -56.7932\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -57.4161\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -58.0447\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -58.6785\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -59.3136\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -59.9547\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -60.6024\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -61.2557\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -61.9143\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -62.5783\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -63.2476\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -63.9227\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -64.6039\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -65.2889\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -65.9800\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -66.6764\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -67.3778\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: -68.0844\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: -68.7989\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -69.5171\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -70.2412\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: -70.9718\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -71.7082\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: -72.4453\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -73.1894\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -73.9349\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: -74.6855\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: -75.4406\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -76.2017\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -76.9689\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -77.7432\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: -78.5252\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -79.2995\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -80.1029\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -80.9007\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -81.7044\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: -82.5149\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: -83.3306\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: -84.1529\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -84.9808\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -85.8158\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: -86.6562\n",
            "Pred: [2, 0, 0, 0, 2, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.5\n",
            "Average accuracy: 0.475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN Experiment"
      ],
      "metadata": {
        "id": "O6FHTezR6fgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_gan = pd.DataFrame(X_train)\n",
        "# data_gan['target'] = y_train\n",
        "data_gan = df.copy()\n",
        "data_gan.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uJ9xq0jGliC-",
        "outputId": "c500226b-1539-4dac-ee84-6931a1ad0d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Size        NB  Albert_2  target\n",
              "0 -0.170559 -0.214897  0.369069       0\n",
              "1 -0.119226 -0.531848  0.271532       0\n",
              "2 -0.000417 -1.827998 -1.403138       1\n",
              "3 -0.233583  0.818416  0.725564       1\n",
              "5  0.733464  0.287587  0.098133       2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb3fe238-e51e-408d-8566-646ca3905840\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Size</th>\n",
              "      <th>NB</th>\n",
              "      <th>Albert_2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.170559</td>\n",
              "      <td>-0.214897</td>\n",
              "      <td>0.369069</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.119226</td>\n",
              "      <td>-0.531848</td>\n",
              "      <td>0.271532</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.000417</td>\n",
              "      <td>-1.827998</td>\n",
              "      <td>-1.403138</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.233583</td>\n",
              "      <td>0.818416</td>\n",
              "      <td>0.725564</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.733464</td>\n",
              "      <td>0.287587</td>\n",
              "      <td>0.098133</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb3fe238-e51e-408d-8566-646ca3905840')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb3fe238-e51e-408d-8566-646ca3905840 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb3fe238-e51e-408d-8566-646ca3905840');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sdv.tabular import CTGAN, CopulaGAN\n",
        "from sdv.evaluation import evaluate\n",
        "\n",
        "generated_data = pd.DataFrame()\n",
        "for i in data_gan['target'].unique():\n",
        "  data_1 = data_gan[data_gan.iloc[:, -1] == i]\n",
        "  d = data_1.copy()\n",
        "  # if len(data_1) < 3:\n",
        "  #   continue\n",
        "  while len(data_1) < 10:\n",
        "    row = d.sample(1)\n",
        "    data_1 = pd.concat([data_1, row], ignore_index=True)\n",
        "  print(data_1)\n",
        "\n",
        "  model = CopulaGAN(\n",
        "              epochs=1500,\n",
        "              batch_size=500,\n",
        "              rounding=6\n",
        "              )\n",
        "\n",
        "  model.fit(data_1)\n",
        "  if i == 0:\n",
        "    new_data = model.sample(150)\n",
        "  else:  \n",
        "    new_data = model.sample(100)\n",
        "  generated_data = pd.concat([generated_data, new_data], ignore_index=True)\n",
        "  print(evaluate(generated_data, data_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcWpCPMnOqML",
        "outputId": "ac20af39-c440-4f7e-e10d-b6014d3e1bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Size        NB  Albert_2  target\n",
            "0  -0.170559 -0.214897  0.369069       0\n",
            "1  -0.119226 -0.531848  0.271532       0\n",
            "9  -0.227333  0.076286 -0.344491       0\n",
            "11 -0.258072  0.400968  0.414130       0\n",
            "18 -0.239556  0.227804 -0.273762       0\n",
            "20 -0.237362 -0.414859  0.054213       0\n",
            "23 -0.136170 -0.539578 -0.580062       0\n",
            "25 -0.258072 -0.552463 -1.541743       0\n",
            "27 -0.007853 -1.992916 -2.372804       0\n",
            "29  0.013191  1.011679  1.041561       0\n",
            "31 -0.113817  0.785432  1.041561       0\n",
            "32 -0.198526  0.504042 -0.726653       0\n",
            "34 -0.263211  0.293771 -0.739772       0\n",
            "36 -0.257033 -1.995493 -2.366529       0\n",
            "39 -0.262114  1.465203  0.953151       0\n",
            "42 -0.262242  1.313684  0.974255       0\n",
            "43 -0.262242  1.313684  0.788878       0\n",
            "0.6518653659572285\n",
            "        Size        NB  Albert_2  target\n",
            "2  -0.000417 -1.827998 -1.403138       1\n",
            "3  -0.233583  0.818416  0.725564       1\n",
            "6  -0.223657 -0.262826 -0.338787       1\n",
            "10 -0.258072  0.787494  0.365077       1\n",
            "12 -0.259689  0.504042  0.216204       1\n",
            "16 -0.244483 -0.149960 -0.285170       1\n",
            "22 -0.025503 -0.980218 -0.185922       1\n",
            "24  0.041498  0.960142  1.349002       1\n",
            "26 -0.159922 -0.866837 -0.549832       1\n",
            "30  0.023707 -2.348520  0.989656       1\n",
            "41 -0.261678  0.949319  0.984522       1\n",
            "0.5041350316264251\n",
            "        Size        NB  Albert_2  target\n",
            "5   0.733464  0.287587  0.098133       2\n",
            "7  -0.232408  1.287401  1.248613       2\n",
            "8  -0.232428  0.766879  1.270859       2\n",
            "13 -0.228739  1.363675  1.327327       2\n",
            "14 -0.238824  0.233473  0.405574       2\n",
            "15 -0.240274 -0.280864 -0.499637       2\n",
            "19 -0.227006 -0.197890 -0.016515       2\n",
            "28 -0.152248 -0.137592  0.697045       2\n",
            "33 -0.040625 -2.425825 -2.851362       2\n",
            "35  0.103014 -0.689035 -0.545269       2\n",
            "37  6.463045  0.649891 -0.872103       2\n",
            "38 -0.007853 -0.727172 -0.454576       2\n",
            "0.48561997596036804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_data = generated_data.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "187DzeXWkzvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, _, y_train, _ = train_test_split(generated_data[['Size', 'NB', 'Albert_2']], generated_data['target'], test_size=1, random_state=1)\n",
        "\n",
        "X_train = generated_data[['Size', 'NB']]\n",
        "y_train = generated_data['target']"
      ],
      "metadata": {
        "id": "WM_M-P5AlfpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts().plot(kind='bar', title='Count (target)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Rt14SAUzBVI-",
        "outputId": "c1dac545-7acb-410a-9b09-227e1e737011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:title={'center':'Count (target)'}>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import statistics\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "models = [\n",
        "    LogisticRegression(multi_class='multinomial'),\n",
        "    XGBClassifier(\n",
        "        max_depth=6,\n",
        "        gamma=2,\n",
        "        eta=0.8,\n",
        "        reg_alpha=0.5,\n",
        "        reg_lambda=0.5\n",
        "      ),\n",
        "    # LogisticRegression(),\n",
        "    svm.SVC(decision_function_shape='ovr', probability=True),\n",
        "    RandomForestClassifier(n_estimators=10),\n",
        "    AdaBoostClassifier(n_estimators=25, learning_rate=2),\n",
        "    # get_model(X_train.shape[1], 1)\n",
        "]\n",
        "\n",
        "# preds = pd.DataFrame()\n",
        "acc_arr = []\n",
        "for i in range(10):\n",
        "  preds = []\n",
        "  weights = [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "  for i, m in enumerate(models):\n",
        "    \n",
        "    m.fit(X_train, y_train)\n",
        "    preds.append(m.predict_proba(X_test[['Size','NB']]) * weights[i])\n",
        "\n",
        "\n",
        "  preds_ensemble = [sum(items)/sum(weights) for items in zip(*preds)]\n",
        "  preds_ensemble = np.argmax(preds_ensemble, axis=1)\n",
        "  # preds_ensemble = le.inverse_transform(preds_ensemble)\n",
        "  print('Pred:', list(preds_ensemble))\n",
        "  acc = accuracy_score(y_test, preds_ensemble)\n",
        "  print('Real:', list(y_test))\n",
        "  print(acc)\n",
        "  acc_arr.append(acc)\n",
        "\n",
        "print(\"Average accuracy:\", statistics.mean(acc_arr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tniL_LhhWRR7",
        "outputId": "9a24b84a-fb15-4223-ddc8-44f7fc4e5da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pred: [0, 0, 2, 0, 0, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.875\n",
            "Pred: [0, 0, 2, 0, 0, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.875\n",
            "Pred: [0, 0, 2, 0, 0, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.875\n",
            "Pred: [0, 0, 2, 0, 0, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.875\n",
            "Pred: [0, 0, 2, 0, 0, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.875\n",
            "Pred: [0, 0, 2, 0, 0, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.875\n",
            "Pred: [0, 0, 2, 0, 0, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.875\n",
            "Pred: [0, 0, 2, 0, 0, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.875\n",
            "Pred: [0, 0, 2, 0, 0, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.875\n",
            "Pred: [0, 0, 2, 0, 0, 0, 0, 2]\n",
            "Real: [0, 0, 2, 1, 0, 0, 0, 2]\n",
            "0.875\n",
            "Average accuracy: 0.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8FS13FyWssJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}